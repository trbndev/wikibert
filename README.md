# ğŸ‘¨ğŸ»â€ğŸ« Wikibert

This is an experimental weekend project for testing purposes.

## ğŸ“ Overview

**Wikibert** is a project that demonstrates text generation using Wikipedia data. It consists of two main components:
- ğŸ A Python script (`get_data.py`) for fetching and saving Wikipedia pages.
- ğŸ““ A Jupyter notebook (`wikibert.ipynb`) for training a text generation model using TensorFlow.

## ğŸ› ï¸ get_data.py

The `get_data.py` script performs the following tasks:
- ğŸ”„ Fetches a Wikipedia page and its linked pages recursively up to a specified depth.
- ğŸ’¾ Saves the content of each page into individual text files in a `data` folder.
- ğŸ§¹ Sanitizes filenames to ensure compatibility with the file system.

## ğŸ“Š wikibert.ipynb

The `wikibert.ipynb` notebook includes the following steps:
- ğŸ” Loads and preprocesses text data from the saved Wikipedia pages.
- ğŸ—ï¸ Builds and trains a GRU-based RNN model for text generation using TensorFlow.
- ğŸ’¾ Saves model checkpoints during training.
- ğŸ“ Generates text using the trained model.

## ğŸš€ Usage

1. Run `get_data.py` to fetch and save Wikipedia pages.
2. Open `wikibert.ipynb` in Jupyter Notebook or Google Colab.
3. Follow the cells in the notebook to train the text generation model and generate text.

## ğŸ“‹ Requirements

- Python 3.x
- `wikipedia-api` library
- TensorFlow
- Jupyter Notebook (for `wikibert.ipynb`)

## ğŸ› ï¸ Installation

Install the required libraries using pip:
```bash
pip install wikipedia-api tensorflow
```

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
